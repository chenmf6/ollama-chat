version: '3.8'

services:
  # Ollama 模型服务
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: unless-stopped
    # command: run llama3.2

  # Node.js 后端服务
  server:
    build:
      context: ./server
      dockerfile: Dockerfile.dev
    container_name: server
    ports:
      - "4000:4000"
    volumes:
      - ./server:/server-app
      - server_node_modules:/server-app/node_modules
    environment:
      - NODE_ENV=development
      - HOST=0.0.0.0
      - PORT=4000
      - CORS_ORIGIN=http://client:3000,http://localhost:3001 # 客户端的地址
      - OLLAMA_HOST=http://ollama:11434  # 配置 Ollama 的主机地址和端口
    depends_on:
      ollama:
        condition: service_started

  # React 前端开发服务器
  client:
    build:
      context: ./client
      dockerfile: Dockerfile.dev
    container_name: client
    ports:
      - "3001:3000" # 将容器的3000端口映射到宿主机的3001端口
    volumes:
      - ./client:/client-app
      - client_node_modules:/client-app/node_modules
    environment:
      - NODE_ENV=development
      - WDS_SOCKET_PORT=3001
      - REACT_APP_SOCKET_URL=http://localhost:4000
    depends_on:
      - server

volumes:
  ollama:
  server_node_modules:
  client_node_modules: